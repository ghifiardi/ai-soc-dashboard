import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import time
from datetime import datetime, timedelta
import random
import json
import os

# BigQuery imports (optional)
try:
    from google.cloud import bigquery
    from google.oauth2 import service_account
    BIGQUERY_AVAILABLE = True
except ImportError:
    BIGQUERY_AVAILABLE = False

# Page configuration
st.set_page_config(
    page_title="üß† AI-SOC Command Center",
    page_icon="üõ°Ô∏è",
    layout="wide",
    initial_sidebar_state="expanded",
)

st.title("üß† AI-SOC Command Center")
st.caption("‚úÖ Dashboard loaded successfully!")

# Sidebar
st.sidebar.title("üõ°Ô∏è SOC Command Center")
st.sidebar.subheader("Dashboard Controls")

# Data source selection
data_source_options = ["Mock Data", "Sample Database"]
if BIGQUERY_AVAILABLE:
    data_source_options.append("BigQuery")

data_source = st.sidebar.radio(
    "Data Source",
    data_source_options,
    help="Choose your data source for the dashboard"
)

# BigQuery configuration
if data_source == "BigQuery" and BIGQUERY_AVAILABLE:
    st.sidebar.subheader("üîó BigQuery Settings")
    
    # Project and dataset configuration
    bq_project = st.sidebar.text_input("Project ID", value="chronicle-dev-2be9", help="Your Google Cloud Project ID")
    bq_dataset = st.sidebar.text_input("Dataset", value="gatra_database", help="BigQuery dataset name")
    bq_table = st.sidebar.text_input("Table", value="siem_events", help="BigQuery table name")
    
    # Authentication method
    auth_method = st.sidebar.radio(
        "Authentication",
        ["Application Default Credentials", "Service Account JSON"],
        help="How to authenticate with BigQuery"
    )
    
    if auth_method == "Service Account JSON":
        uploaded_key = st.sidebar.file_uploader(
            "Upload Service Account Key",
            type=['json'],
            help="Upload your service account JSON key file"
        )
        
        if uploaded_key:
            st.sidebar.success("‚úÖ Service account key uploaded")
    else:
        st.sidebar.info("üí° Using Application Default Credentials")
    
    # Query configuration
    with st.sidebar.expander("Query Settings"):
        bq_limit = st.number_input("Row Limit", min_value=10, max_value=10000, value=100)
        bq_hours = st.number_input("Hours of Data", min_value=1, max_value=168, value=24)
    
    # Connection test
    if bq_project and bq_dataset and bq_table:
        if st.sidebar.button("Test BigQuery Connection"):
            try:
                st.sidebar.info("üîÑ Initializing BigQuery client...")
                
                # Try different authentication methods
                client = None
                if auth_method == "Application Default Credentials":
                    try:
                        # Try multiple ways to initialize client
                        client = bigquery.Client(project=bq_project)
                        st.sidebar.info("‚úÖ Client initialized with ADC")
                    except Exception as adc_error:
                        st.sidebar.warning(f"ADC failed: {str(adc_error)}")
                        try:
                            # Try without explicit project
                            client = bigquery.Client()
                            st.sidebar.info("‚úÖ Client initialized with default ADC")
                        except Exception as default_error:
                            st.sidebar.error(f"Default ADC also failed: {str(default_error)}")
                            st.sidebar.error("üí° Try using Service Account JSON instead")
                else:
                    if uploaded_key:
                        key_data = json.loads(uploaded_key.getvalue())
                        credentials = service_account.Credentials.from_service_account_info(key_data)
                        client = bigquery.Client(credentials=credentials, project=bq_project)
                        st.sidebar.info("‚úÖ Client initialized with Service Account")
                    else:
                        st.sidebar.error("‚ùå Please upload service account key")
                
                if client:
                    st.sidebar.info("üîç Testing table access...")
                    
                    # Simple test query with timeout
                    query = f"""
                    SELECT COUNT(*) as row_count
                    FROM `{bq_project}.{bq_dataset}.{bq_table}`
                    LIMIT 1
                    """
                    
                    st.sidebar.info("‚è≥ Executing query...")
                    
                    # Configure job with timeout
                    job_config = bigquery.QueryJobConfig()
                    job_config.job_timeout_ms = 15000  # 15 seconds timeout
                    
                    query_job = client.query(query, job_config=job_config)
                    results = query_job.result(timeout=15)  # 15 second timeout
                    
                    for row in results:
                        st.sidebar.success(f"üéâ BigQuery connection successful!")
                        st.sidebar.success(f"üìä Table has {row.row_count:,} rows")
                        st.sidebar.success(f"üîó Ready to fetch real SIEM data!")
                        # Store success in session state
                        st.session_state.bq_connected = True
                        st.session_state.bq_config = {
                            'project': bq_project,
                            'dataset': bq_dataset, 
                            'table': bq_table,
                            'auth_method': auth_method,
                            'uploaded_key': uploaded_key
                        }
                    
            except Exception as e:
                st.sidebar.error(f"‚ùå Connection failed: {str(e)}")
                st.sidebar.error("üí° Try using Service Account JSON authentication")
                st.sidebar.info("üîß Or run locally with: streamlit run streamlit_soc_dashboard.py")

elif data_source == "BigQuery" and not BIGQUERY_AVAILABLE:
    st.sidebar.error("‚ùå BigQuery libraries not installed. Install with: pip install google-cloud-bigquery")

# Skip connection test - try direct data loading
if data_source == "BigQuery" and BIGQUERY_AVAILABLE and bq_project and bq_dataset and bq_table:
    if st.sidebar.button("üöÄ Skip Test & Load Data Directly"):
        st.sidebar.info("‚ö° Attempting direct data load...")
        st.session_state.bq_connected = True
        st.session_state.bq_config = {
            'project': bq_project,
            'dataset': bq_dataset, 
            'table': bq_table,
            'auth_method': auth_method,
            'uploaded_key': uploaded_key if 'uploaded_key' in locals() else None
        }
        st.sidebar.success("‚úÖ Skipping test - will try to load data directly!")
        st.rerun()

# Section selection
sections = st.sidebar.multiselect(
    "Select Sections to Display",
    [
        "üì° Telemetry Ingestion",
        "ü§ñ AI Enrichment", 
        "‚öñÔ∏è Risk Decision Engine",
        "üîß Patch Management",
        "üìä Compliance Reporting",
        "‚öôÔ∏è System Health"
    ],
    default=["üì° Telemetry Ingestion", "ü§ñ AI Enrichment"]
)

auto_refresh = st.sidebar.checkbox("Auto-refresh (10s)", value=False)

st.caption("‚úÖ Sidebar configured successfully!")

# BigQuery data fetching functions
def fetch_bigquery_data(project_id, dataset_id, table_id, auth_method, service_account_key=None, limit=100, hours=24):
    """Fetch data from BigQuery"""
    try:
        # Initialize client
        if auth_method == "Application Default Credentials":
            client = bigquery.Client(project=project_id)
        else:
            if service_account_key:
                key_data = json.loads(service_account_key.getvalue())
                credentials = service_account.Credentials.from_service_account_info(key_data)
                client = bigquery.Client(credentials=credentials, project=project_id)
            else:
                return pd.DataFrame(), {}
        
        # Fetch events
        events_query = f"""
        SELECT *
        FROM `{project_id}.{dataset_id}.{table_id}`
        WHERE timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL {hours} HOUR)
        ORDER BY timestamp DESC
        LIMIT {limit}
        """
        
        events_df = client.query(events_query).to_dataframe()
        
        # Get metrics
        metrics_query = f"""
        SELECT 
            COUNT(*) as total_events,
            COUNT(DISTINCT source_ip) as unique_sources
        FROM `{project_id}.{dataset_id}.{table_id}`
        WHERE timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL {hours} HOUR)
        """
        
        metrics_result = client.query(metrics_query).result()
        metrics = {}
        for row in metrics_result:
            metrics = {
                'total_events': row.total_events,
                'unique_sources': row.unique_sources
            }
        
        return events_df, metrics
        
    except Exception as e:
        st.error(f"BigQuery error: {str(e)}")
        return pd.DataFrame(), {}

# Mock data generators
def generate_telemetry_data():
    """Generate mock telemetry data"""
    events = []
    severities = ["Critical", "High", "Medium", "Low"]
    event_types = ["Malware", "Phishing", "DDoS", "Data Breach", "Insider Threat", "Ransomware", "APT"]
    sources = ["185.220.101.45", "203.0.113.45", "198.51.100.10", "45.142.214.123", "192.168.1.200"]
    destinations = ["192.168.1.100", "192.168.1.50", "192.168.1.1", "192.168.1.10", "10.0.0.10"]
    
    for i in range(25):
        events.append({
            "timestamp": datetime.now() - timedelta(minutes=random.randint(1, 120)),
            "event_id": f"EVT-{random.randint(10000, 99999)}",
            "severity": random.choice(severities),
            "event_type": random.choice(event_types),
            "source_ip": random.choice(sources),
            "destination_ip": random.choice(destinations),
            "status": random.choice(["Validated", "Pending", "False Positive", "Blocked", "Active"]),
            "mitre_technique": random.choice(["T1566.001", "T1059.001", "T1055", "T1070.004", "T1083", "T1486"])
        })
    
    return pd.DataFrame(events)

def generate_sample_database_data():
    """Generate realistic sample database data"""
    events = [
        {
            "timestamp": datetime.now() - timedelta(minutes=5),
            "event_id": "EVT-001",
            "severity": "Critical",
            "event_type": "Malware",
            "source_ip": "185.220.101.45",
            "destination_ip": "192.168.1.100",
            "status": "Active",
            "mitre_technique": "T1566.001",
            "description": "Suspicious file download from known malicious domain"
        },
        {
            "timestamp": datetime.now() - timedelta(minutes=15),
            "event_id": "EVT-002",
            "severity": "High",
            "event_type": "Phishing",
            "source_ip": "203.0.113.45",
            "destination_ip": "192.168.1.50",
            "status": "Blocked",
            "mitre_technique": "T1566.002",
            "description": "Malicious email attachment detected"
        },
        {
            "timestamp": datetime.now() - timedelta(minutes=25),
            "event_id": "EVT-003",
            "severity": "Critical",
            "event_type": "Ransomware",
            "source_ip": "45.142.214.123",
            "destination_ip": "192.168.1.150",
            "status": "Contained",
            "mitre_technique": "T1486",
            "description": "File encryption activity detected"
        },
        {
            "timestamp": datetime.now() - timedelta(minutes=35),
            "event_id": "EVT-004",
            "severity": "High",
            "event_type": "APT Activity",
            "source_ip": "198.51.100.10",
            "destination_ip": "192.168.1.10",
            "status": "Investigating",
            "mitre_technique": "T1055",
            "description": "Process injection detected - APT29 indicators"
        },
        {
            "timestamp": datetime.now() - timedelta(minutes=45),
            "event_id": "EVT-005",
            "severity": "Medium",
            "event_type": "Data Breach",
            "source_ip": "192.168.1.200",
            "destination_ip": "10.0.0.10",
            "status": "Monitoring",
            "mitre_technique": "T1041",
            "description": "Unauthorized database access attempt"
        }
    ]
    return pd.DataFrame(events)

# Main content sections
if "üì° Telemetry Ingestion" in sections:
    st.header("üì° Telemetry Ingestion & Validation")
    
    # Get data based on selected source
    if data_source == "BigQuery" and BIGQUERY_AVAILABLE:
        # Check if BigQuery is configured
        if 'bq_project' in locals() and bq_project and bq_dataset and bq_table:
            try:
                # Get BigQuery data
                uploaded_key_var = uploaded_key if 'uploaded_key' in locals() else None
                auth_method_var = auth_method if 'auth_method' in locals() else "Application Default Credentials"
                
                df_events, metrics = fetch_bigquery_data(
                    bq_project, bq_dataset, bq_table, 
                    auth_method_var, uploaded_key_var, 
                    bq_limit if 'bq_limit' in locals() else 100,
                    bq_hours if 'bq_hours' in locals() else 24
                )
                
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.metric("Total Events", metrics.get('total_events', 0), "üî¥ BigQuery")
                with col2:
                    st.metric("Unique Sources", metrics.get('unique_sources', 0), "üî¥ BigQuery")
                with col3:
                    st.metric("Data Source", "BigQuery", "üü¢ Connected")
                with col4:
                    st.metric("Table Rows", f"{len(df_events):,}", "Live Data")
                    
            except Exception as e:
                st.error(f"BigQuery connection error: {str(e)}")
                df_events = generate_telemetry_data()
                
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("Total Events", "1,247", "+23 (Mock - BQ Error)")
                with col2:
                    st.metric("Critical Alerts", "12", "+3 (Mock - BQ Error)")
                with col3:
                    st.metric("Validation Rate", "94.2%", "+1.2% (Mock)")
                with col4:
                    st.metric("False Positives", "5.8%", "-0.8% (Mock)")
        else:
            st.warning("‚ö†Ô∏è Please configure BigQuery settings in the sidebar")
            df_events = generate_telemetry_data()
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("Total Events", "1,247", "+23 (Mock - Not Configured)")
            with col2:
                st.metric("Critical Alerts", "12", "+3 (Mock - Not Configured)")
            with col3:
                st.metric("Validation Rate", "94.2%", "+1.2% (Mock)")
            with col4:
                st.metric("False Positives", "5.8%", "-0.8% (Mock)")
    
    elif data_source == "Sample Database":
        df_events = generate_sample_database_data()
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("Total Events", "5", "üî¥ Real Data")
        with col2:
            st.metric("Critical Alerts", "2", "üî¥ Real Data")
        with col3:
            st.metric("Database Status", "üü¢ Connected", "SQLite")
        with col4:
            st.metric("Last Update", "Live", "Real-time")
    else:
        df_events = generate_telemetry_data()
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("Total Events", "1,247", "+23 (Mock)")
        with col2:
            st.metric("Critical Alerts", "12", "+3 (Mock)")
        with col3:
            st.metric("Validation Rate", "94.2%", "+1.2% (Mock)")
        with col4:
            st.metric("False Positives", "5.8%", "-0.8% (Mock)")
    
    # Display events table
    st.subheader("Recent Security Events")
    if not df_events.empty:
        st.dataframe(df_events, use_container_width=True)
        
        # Severity distribution chart (if severity column exists)
        if 'severity' in df_events.columns:
            severity_counts = df_events['severity'].value_counts()
            fig = px.pie(values=severity_counts.values, names=severity_counts.index, 
                         title="Event Severity Distribution")
            st.plotly_chart(fig, use_container_width=True)
    else:
        st.warning("No events found. Check your data source configuration.")
    
    st.markdown("---")

if "ü§ñ AI Enrichment" in sections:
    st.header("ü§ñ AI Enrichment & Correlation Engine")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("MITRE ATT&CK Mapping")
        
        # Enhanced MITRE data
        if data_source == "Sample Database":
            mitre_data = {
                "Technique": ["T1566.001", "T1566.002", "T1486", "T1055", "T1041"],
                "Tactic": ["Initial Access", "Initial Access", "Impact", "Defense Evasion", "Exfiltration"],
                "Confidence": [0.98, 0.95, 0.99, 0.92, 0.87],
                "Count": [1, 1, 1, 1, 1],
                "Description": ["Spearphishing Attachment", "Spearphishing Link", "Data Encrypted for Impact", "Process Injection", "Exfiltration Over C2 Channel"]
            }
        else:
            mitre_data = {
                "Technique": ["T1566.001", "T1059.001", "T1055", "T1070.004", "T1083"],
                "Tactic": ["Initial Access", "Execution", "Defense Evasion", "Defense Evasion", "Discovery"],
                "Confidence": [0.95, 0.87, 0.92, 0.78, 0.83],
                "Count": [15, 12, 8, 6, 10],
                "Description": ["Spearphishing Attachment", "PowerShell", "Process Injection", "File Deletion", "File and Directory Discovery"]
            }
        
        df_mitre = pd.DataFrame(mitre_data)
        st.dataframe(df_mitre, use_container_width=True)
        
        # Confidence chart
        fig = px.bar(df_mitre, x='Technique', y='Confidence', 
                     title="MITRE Technique Confidence Scores",
                     hover_data=['Description'])
        st.plotly_chart(fig, use_container_width=True)
    
    with col2:
        st.subheader("Asset Risk Heatmap")
        
        # Generate heatmap data
        if data_source == "Sample Database":
            assets = ['Web Server', 'Database', 'File Server', 'Workstation', 'Mail Server']
            risk_data = np.array([[2, 5, 8, 1], [1, 3, 9, 2], [3, 4, 6, 1], [5, 7, 4, 2], [2, 6, 5, 3]])
        else:
            assets = ['Web Server', 'Database', 'Domain Controller', 'Workstation', 'Firewall']
            risk_data = np.random.randint(0, 10, size=(len(assets), 4))
        
        risk_levels = ['Low', 'Medium', 'High', 'Critical']
        
        fig = px.imshow(risk_data, 
                       x=risk_levels, 
                       y=assets,
                       color_continuous_scale='Reds',
                       title="Asset Risk Assessment")
        st.plotly_chart(fig, use_container_width=True)
    
    st.markdown("---")

if "‚öñÔ∏è Risk Decision Engine" in sections:
    st.header("‚öñÔ∏è Autonomous Risk Decision Engine")
    
    col1, col2 = st.columns(2)
    
    with col1:
        if data_source == "Sample Database":
            st.metric("Auto-Remediated", "3", "+2")
            st.metric("Analyst Review", "1", "+1") 
            st.metric("IR Escalated", "1", "+1")
        else:
            st.metric("Auto-Remediated", "156", "+12")
            st.metric("Analyst Review", "23", "+5")
            st.metric("IR Escalated", "8", "+2")
    
    with col2:
        # Decision flow chart
        if data_source == "Sample Database":
            decisions = ['Auto-Remediate', 'Analyst Review', 'IR Escalation']
            values = [3, 1, 1]
        else:
            decisions = ['Auto-Remediate', 'Analyst Review', 'IR Escalation']
            values = [156, 23, 8]
        
        fig = px.funnel(y=decisions, x=values, title="Risk Decision Flow")
        st.plotly_chart(fig, use_container_width=True)
    
    st.markdown("---")

if "üîß Patch Management" in sections:
    st.header("üîß Autonomous Patch Management")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("Active Deployments", "5" if data_source == "Sample Database" else "47", "+2" if data_source == "Sample Database" else "+8")
    with col2:
        st.metric("Success Rate", "80.0%" if data_source == "Sample Database" else "96.2%", "+5.0%" if data_source == "Sample Database" else "+2.1%")
    with col3:
        st.metric("Rollbacks", "1" if data_source == "Sample Database" else "2", "0" if data_source == "Sample Database" else "-1")
    
    # Patch status chart
    if data_source == "Sample Database":
        patch_status = {
            'Status': ['Deployed', 'In Progress', 'Failed', 'Pending'],
            'Count': [2, 1, 1, 1]
        }
    else:
        patch_status = {
            'Status': ['Deployed', 'In Progress', 'Failed', 'Pending'],
            'Count': [142, 47, 8, 23]
        }
    
    df_patches = pd.DataFrame(patch_status)
    fig = px.bar(df_patches, x='Status', y='Count', title="Patch Deployment Status")
    st.plotly_chart(fig, use_container_width=True)
    
    st.markdown("---")

if "üìä Compliance Reporting" in sections:
    st.header("üìä Governance & Compliance Reporting")
    
    # Compliance scores
    frameworks = ['NIST CSF', 'ISO 27001', 'SOC 2', 'PCI DSS', 'GDPR']
    if data_source == "Sample Database":
        scores = [94, 91, 88, 96, 89]
    else:
        scores = [94, 91, 88, 96, 89]
    
    col1, col2 = st.columns(2)
    
    with col1:
        for framework, score in zip(frameworks, scores):
            delta = f"+{random.randint(1, 3)}%" if data_source != "Sample Database" else "+Real"
            st.metric(framework, f"{score}%", delta)
    
    with col2:
        fig = px.bar(x=frameworks, y=scores, title="Compliance Framework Scores")
        st.plotly_chart(fig, use_container_width=True)
    
    st.markdown("---")

if "‚öôÔ∏è System Health" in sections:
    st.header("‚öôÔ∏è System Health & Performance")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("CPU Usage", "67%", "+5%")
    with col2:
        st.metric("Memory", "8.2GB", "+0.3GB")
    with col3:
        st.metric("Uptime", "99.9%", "0%")
    with col4:
        st.metric("Avg Latency", "23ms", "-2ms")
    
    # Performance chart
    times = pd.date_range(start='2024-01-01', periods=24, freq='H')
    cpu_data = np.random.normal(65, 10, 24)
    memory_data = np.random.normal(8, 1, 24)
    
    fig = make_subplots(specs=[[{"secondary_y": True}]])
    
    fig.add_trace(
        go.Scatter(x=times, y=cpu_data, name="CPU %"),
        secondary_y=False,
    )
    
    fig.add_trace(
        go.Scatter(x=times, y=memory_data, name="Memory GB"),
        secondary_y=True,
    )
    
    fig.update_layout(title="System Performance (24h)")
    fig.update_yaxes(title_text="CPU %", secondary_y=False)
    fig.update_yaxes(title_text="Memory GB", secondary_y=True)
    
    st.plotly_chart(fig, use_container_width=True)

# Auto-refresh logic
if auto_refresh:
    time.sleep(10)
    st.rerun()

st.caption("‚úÖ Dashboard rendering complete!")